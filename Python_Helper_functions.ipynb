{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9djl+UM8pv45XMQ1sIBzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkusSchilling/py_scripts/blob/main/Python_Helper_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Helper Functions\n",
        "\n",
        "This notebook is supposed to collect some functions that may come up handy at some time when trying to write some scripts. This collection may never be finished but it also does raise no claim to completeness, at all."
      ],
      "metadata": {
        "id": "DmA0FMX2c6_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation & Imports of relevant packages\n",
        "\n",
        "Occasionally, the list of package installations and imports may have to be updated, depending on the requirements set by the functions that are defined below."
      ],
      "metadata": {
        "id": "XXyJLHUUdkiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages that need to installed (for the sake of completeness, there are a lot of packages given, here.)\n",
        "%pip install rdflib\n",
        "%pip install sparqlwrapper"
      ],
      "metadata": {
        "id": "dZzyVWcWdPO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports that need to performed (for the sake of completeness, there are a lot of imports given, here.)\n",
        "from rdflib import Graph, Namespace, URIRef, Literal, XSD, RDF, RDFS, PROV, OWL, DC\n",
        "from rdflib.term import Identifier\n",
        "from rdflib.collection import Collection\n",
        "from rdflib.namespace import RDF, RDFS, SKOS, XSD, OWL\n",
        "import rdflib.plugins.sparql.update\n",
        "import pandas as pd\n",
        "import io\n",
        "from io import StringIO\n",
        "import urllib.parse\n",
        "from IPython.display import display, Markdown, HTML, JSON\n",
        "from scipy.spatial import Delaunay\n",
        "from scipy.spatial.distance import euclidean\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import difflib"
      ],
      "metadata": {
        "id": "3phFqpkYdZBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "Rto4-IRSdsRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find differences in strings"
      ],
      "metadata": {
        "id": "1TGRtHgFeTZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5PKKcMQZOVU"
      },
      "outputs": [],
      "source": [
        "# Function to find differences in two strings\n",
        "import difflib\n",
        "\n",
        "cases=[(\"String 1\", \"String 2\")]\n",
        "# Instead of strings given in \"\", lists and dataframes (variables) can also be used, such as: cases=[(CSV, CSV2)]\n",
        "\n",
        "for a,b in cases:\n",
        "    print('{} => {}'.format(a,b))\n",
        "    for i,s in enumerate(difflib.ndiff(a, b)):\n",
        "        if s[0]==' ': continue\n",
        "        elif s[0]=='-':\n",
        "            print(u'Delete \"{}\" from position {}'.format(s[-1],i))\n",
        "        elif s[0]=='+':\n",
        "            print(u'Add \"{}\" to position {}'.format(s[-1],i))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Counter"
      ],
      "metadata": {
        "id": "XCvfzXyUeNJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a global counter possibly usable for the creation of arbitrary instance names (numbers)\n",
        "instanceCounter = 0\n",
        "def nextInstanceNum():\n",
        "    global instanceCounter\n",
        "    instanceCounter = instanceCounter + 1\n",
        "    return str(instanceCounter)"
      ],
      "metadata": {
        "id": "xezzOj4SZ29C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method 'add' used to write triples to an RDF graph\n",
        "def add(s,p,o):\n",
        "\n",
        "    # in this case p is \"ObjectProperty\"\n",
        "    if o.find('http://')==0 or o.find('https://')==0:\n",
        "        g.add( (URIRef(s), URIRef(p), URIRef(o)) )\n",
        "\n",
        "    # in this case p is \"DatatypeProperty\"\n",
        "    else:\n",
        "        # if we can parse o as Float, just set the datatype\n",
        "        try:\n",
        "            g.add( (URIRef(s), URIRef(p), Literal(float(o), datatype=XSD.float) ))\n",
        "        except:\n",
        "            g.add( (URIRef(s), URIRef(p), Literal(o) ))"
      ],
      "metadata": {
        "id": "LNHZFAWEZ5_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in CSV data"
      ],
      "metadata": {
        "id": "pNfOVPQdea_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in CSV data\n",
        "\n",
        "CSV = open(\"test.csv\").read()\n",
        "# CSV dataset namehas to be set, accordingly. Function will store data as a string!\n",
        "\n",
        "# Store data directly to a dataframe\n",
        "import pandas as pd\n",
        "CSV_df = pd.read_csv('test.csv')\n",
        "# CSV dataset namehas to be set, accordingly.\n",
        "\n",
        "# Also possible: load data from the 'CSV-string' created above to a dataframe\n",
        "data = pd.read_csv(io.StringIO(CSV), sep=';')"
      ],
      "metadata": {
        "id": "wMZdZfQJaNJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change standard output to a file of choice\n",
        "import sys\n",
        "old_stdout = sys.stdout\n",
        "sys.stdout = open('output-file.csv', 'w') # Outpul file name AND format have to be changed, accordingly."
      ],
      "metadata": {
        "id": "z-BerUJabfnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change the standard output file in Jupyter Notebook to a file of choice"
      ],
      "metadata": {
        "id": "vc5HOXK2eeg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to write strings to a CSV line by line\n",
        "text = \"This is an example., This is another example for the second line. \"\n",
        "# Set ',' as separator for 1 line; set ';' as separator for the next column.\n",
        "\n",
        "s = StringIO(text)\n",
        "with open('fileName.csv', 'w') as f:\n",
        "    for line in s:\n",
        "        f.write(line)\n",
        "\n",
        "# The fileName has to be set, accordingly."
      ],
      "metadata": {
        "id": "6ee3mwQKcPet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare strings (case sensitive)"
      ],
      "metadata": {
        "id": "nFSEbWNSeo5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to compare strings (case sensitive)\n",
        "string1 = \"Abrar\"\n",
        "string2 = \"Ahmed\"\n",
        "string3 = \"ABCD\"\n",
        "string4 = \"ABCD\"\n",
        "if string1 <= string2:\n",
        "    print(string1,\" is smaller \",string2,\" is greater\")\n",
        "if string2 >= string4:\n",
        "    print(string4,\" is smaller \", string2,\" is greater\")\n",
        "if string3 == string4:\n",
        "    print(string3,\" is equal to \",string4)\n",
        "if string1 != string3:\n",
        "    print(string1,\" is not equal to \", string3)\n",
        "\n",
        "# The second method is to use a dedicated string function to perform comparisons,\n",
        "# the __eq__() function. It is a magic function defined in the string class and\n",
        "# compares two strings to return True if they are equal or Fale if they are not.\n",
        "\n",
        "# if s1.__eq__(s2):\n",
        "  #  print('s1 and s2 are equal.')"
      ],
      "metadata": {
        "id": "BU3-DfyPfR-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare strings (case insensitive)"
      ],
      "metadata": {
        "id": "HrbzSebjevQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to compare strings (case insensitive)\n",
        "s1 = 'String'\n",
        "s2 = 'String'\n",
        "s3 = 'string'\n",
        "\n",
        "if s1.casefold() == s3.casefold():\n",
        "    print(s1.casefold())\n",
        "    print(s3.casefold())\n",
        "    print('s1 and s3 are equal in case-insensitive comparison')\n",
        "\n",
        "if s1.lower() == s3.lower():\n",
        "    print(s1.lower())\n",
        "    print(s3.lower())\n",
        "    print('s1 and s3 are equal in case-insensitive comparison')\n",
        "\n",
        "if s1.upper() == s3.upper():\n",
        "    print(s1.upper())\n",
        "    print(s3.upper())\n",
        "    print('s1 and s3 are equal in case-insensitive comparison')"
      ],
      "metadata": {
        "id": "MMezCuiafuxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfom Inputs to IRIs (Ontology development)"
      ],
      "metadata": {
        "id": "RZCRgWPjezb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transform inputs to IRIs.\n",
        "def to_iri(input):\n",
        "    try:\n",
        "        return input.iri\n",
        "    except:\n",
        "        pass\n",
        "    return input\n",
        "\n",
        "# Function to write the result of a SPARQL query into a (pandas) data frame.\n",
        "import pandas as pd\n",
        "def sparql_result_to_df(res):\n",
        "    l = []\n",
        "    for row in res:\n",
        "        r = [ to_iri(item)  for item in row]\n",
        "        l.append(r)\n",
        "    return pd.DataFrame(l)"
      ],
      "metadata": {
        "id": "Pnm76ZGNoomJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triangulation to get the average distances between X and Y coordinates\n",
        "\n",
        "This function originated from an analysis of precipitates as a result of a heat treatment of an aluminium alloy. The result will also be plotted when using this function."
      ],
      "metadata": {
        "id": "QH7HuIYZe6cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for triangulation to get the average precipitate distance out of X and Y coordinates in dependence on the material designation and the images taken.\n",
        "\n",
        "MATL_DESIGNATION = 1\n",
        "# MATL_DESIGNATION_LABEL = 2\n",
        "IMG = 4\n",
        "XPOSITION = 6\n",
        "YPOSITION = 8\n",
        "plt.subplots(figsize=(12., 12.))\n",
        "cmap = plt.get_cmap(\"Purples\")\n",
        "lambdas_50pct = dict()\n",
        "for matl_designation, group in df.groupby(by=[MATL_DESIGNATION]):\n",
        "    result = []\n",
        "    for img, subgroup in group.groupby(by=[IMG]):\n",
        "      tup = subgroup[YPOSITION], subgroup[XPOSITION]\n",
        "      point_coords = np.vstack(tup).T\n",
        "      tri = Delaunay(point_coords)\n",
        "      for simplex in tri.simplices:\n",
        "          #Compute the three distances of the points 1-2, 1-3, 2-3\n",
        "          for i,j in ((0,1), (0, 2), (1,2)):\n",
        "              point = (point_coords[simplex[i]], point_coords[simplex[j]])\n",
        "              result.append(euclidean(point[0], point[1]))\n",
        "    X = np.sort(result)\n",
        "    X *= 0.15 #nm per pixel\n",
        "    Y = np.linspace(0., 1., num=len(X))\n",
        "    lambda_50pct = X[int(len(X)/2)]\n",
        "    lambdas_50pct[matl_designation] = lambda_50pct\n",
        "    print(matl_designation, \", approx 50% quantil: {lambda_50pct:.1f}\".format(lambda_50pct=lambda_50pct))\n",
        "    plt.plot(X, Y, '.', label=(matl_designation + \"lambda=\", str(lambda_50pct)))\n",
        "\n",
        "plt.xlabel(\"Distance in nm\")\n",
        "plt.ylabel(\"Quantile\")\n",
        "# plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QvROfK34ykuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}